{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "704cffa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D, Input\n",
    "from tensorflow.keras.layers import Flatten, add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e2de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "epochs=200\n",
    "data_augmentation=True\n",
    "num_classes=10\n",
    "\n",
    "subtract_pixel_mean=True\n",
    "\n",
    "n=3\n",
    "version=1\n",
    "\n",
    "if version == 1:\n",
    "    depth = n * 6 + 2\n",
    "elif version == 2:\n",
    "    depth = n * 9 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "959962c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'ResNet%dv%d' % (depth, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baf92036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170498071/170498071 [==============================] - 227s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0ab774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a8ea3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n",
      "y_train shape: (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "# if subtract pixel mean is enabled\n",
    "if subtract_pixel_mean:\n",
    "    x_train_mean = np.mean(x_train, axis=0)\n",
    "    x_train -= x_train_mean\n",
    "    x_test -= x_train_mean\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe73c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26fa199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    print('Learning rate: ', lr)\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9341a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30455ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v1(input_shape, depth, num_classes=10):\n",
    "\n",
    "    if (depth - 2) % 6 != 0:\n",
    "        raise ValueError('depth should be 6n+2 (eg 20, 32, in [a])')\n",
    "    # start model definition.\n",
    "    num_filters = 16\n",
    "    num_res_blocks = int((depth - 2) / 6)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = resnet_layer(inputs=inputs)\n",
    "    # instantiate the stack of residual units\n",
    "    for stack in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            # first layer but not first stack\n",
    "            if stack > 0 and res_block == 0:  \n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters,\n",
    "                             activation=None)\n",
    "            # first layer but not first stack\n",
    "            if stack > 0 and res_block == 0:\n",
    "                # linear projection residual shortcut\n",
    "                # connection to match changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    "        num_filters *= 2\n",
    "\n",
    "    # add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd9aadfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_v2(input_shape, depth, num_classes=10):\n",
    "    \n",
    "    if (depth - 2) % 9 != 0:\n",
    "        raise ValueError('depth should be 9n+2 (eg 110 in [b])')\n",
    "    # start model definition.\n",
    "    num_filters_in = 16\n",
    "    num_res_blocks = int((depth - 2) / 9)\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # v2 performs Conv2D with BN-ReLU\n",
    "    # on input before splitting into 2 paths\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=num_filters_in,\n",
    "                     conv_first=True)\n",
    "\n",
    "    # instantiate the stack of residual units\n",
    "    for stage in range(3):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                # first layer and first stage\n",
    "                if res_block == 0:  \n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                # first layer but not first stage\n",
    "                if res_block == 0:\n",
    "                    # downsample\n",
    "                    strides = 2 \n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection\n",
    "                # to match changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=8)(x)\n",
    "    y = Flatten()(x)\n",
    "    outputs = Dense(num_classes,\n",
    "                    activation='softmax',\n",
    "                    kernel_initializer='he_normal')(y)\n",
    "\n",
    "    # instantiate model.\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adc23ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 16)   448         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 16)  64          ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 16)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 16)   2320        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 32, 32, 16)   0           ['activation[0][0]',             \n",
      "                                                                  'batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 32, 32, 16)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 32, 32, 16)   0           ['activation_2[0][0]',           \n",
      "                                                                  'batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 32, 32, 16)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 32, 32, 16)   0           ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 16)   2320        ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 32, 32, 16)  64          ['conv2d_6[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 32, 32, 16)   0           ['activation_4[0][0]',           \n",
      "                                                                  'batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 32, 32, 16)   0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 16, 16, 32)   4640        ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_7[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 16, 16, 32)   9248        ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 16, 16, 32)   544         ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_8[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 16, 16, 32)   0           ['conv2d_9[0][0]',               \n",
      "                                                                  'batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 16, 16, 32)   0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_8[0][0]']           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 16, 16, 32)  128         ['conv2d_10[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 16, 16, 32)   0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 16, 16, 32)  128         ['conv2d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 16, 16, 32)   0           ['activation_8[0][0]',           \n",
      "                                                                  'batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " activation_10 (Activation)     (None, 16, 16, 32)   0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_10[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 16, 16, 32)  128         ['conv2d_12[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_11 (Activation)     (None, 16, 16, 32)   0           ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 16, 16, 32)   9248        ['activation_11[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 16, 16, 32)  128         ['conv2d_13[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 16, 16, 32)   0           ['activation_10[0][0]',          \n",
      "                                                                  'batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " activation_12 (Activation)     (None, 16, 16, 32)   0           ['add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 8, 8, 64)     18496       ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 8, 8, 64)    256         ['conv2d_14[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_13 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_13[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 8, 8, 64)     2112        ['activation_12[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 8, 8, 64)    256         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 8, 8, 64)     0           ['conv2d_16[0][0]',              \n",
      "                                                                  'batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " activation_14 (Activation)     (None, 8, 8, 64)     0           ['add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_14[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 8, 8, 64)    256         ['conv2d_17[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_15 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_15[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 8, 8, 64)    256         ['conv2d_18[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 8, 8, 64)     0           ['activation_14[0][0]',          \n",
      "                                                                  'batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " activation_16 (Activation)     (None, 8, 8, 64)     0           ['add_7[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_16[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 8, 8, 64)    256         ['conv2d_19[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_17 (Activation)     (None, 8, 8, 64)     0           ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 8, 8, 64)     36928       ['activation_17[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 8, 8, 64)    256         ['conv2d_20[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " add_8 (Add)                    (None, 8, 8, 64)     0           ['activation_16[0][0]',          \n",
      "                                                                  'batch_normalization_18[0][0]'] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " activation_18 (Activation)     (None, 8, 8, 64)     0           ['add_8[0][0]']                  \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 1, 1, 64)    0           ['activation_18[0][0]']          \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 64)           0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           650         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 274,442\n",
      "Trainable params: 273,066\n",
      "Non-trainable params: 1,376\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarim\\anaconda3\\envs\\work\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "if version == 2:\n",
    "    model = resnet_v2(input_shape=input_shape, depth=depth)\n",
    "else:\n",
    "    model = resnet_v1(input_shape=input_shape, depth=depth)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(lr=lr_schedule(0)),\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d095267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet20v1\n"
     ]
    }
   ],
   "source": [
    "print(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2b91055",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "filepath = os.path.join(save_dir, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81fff831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare callbacks for model saving and for learning rate adjustment.\n",
    "checkpoint = ModelCheckpoint(filepath=filepath,\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True)\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "\n",
    "callbacks = [checkpoint, lr_reducer, lr_scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "358099ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n",
      "Learning rate:  0.001\n",
      "Epoch 1/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.6377 - acc: 0.4630\n",
      "Epoch 1: val_acc improved from -inf to 0.47250, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.001.h5\n",
      "391/391 [==============================] - 30s 62ms/step - loss: 1.6377 - acc: 0.4630 - val_loss: 1.6323 - val_acc: 0.4725 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 2/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 1.2642 - acc: 0.5979\n",
      "Epoch 2: val_acc improved from 0.47250 to 0.59860, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.002.h5\n",
      "391/391 [==============================] - 23s 59ms/step - loss: 1.2642 - acc: 0.5979 - val_loss: 1.2790 - val_acc: 0.5986 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 3/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 1.0827 - acc: 0.6680\n",
      "Epoch 3: val_acc improved from 0.59860 to 0.63210, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.003.h5\n",
      "391/391 [==============================] - 23s 60ms/step - loss: 1.0827 - acc: 0.6680 - val_loss: 1.2035 - val_acc: 0.6321 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 4/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.9601 - acc: 0.7161\n",
      "Epoch 4: val_acc improved from 0.63210 to 0.64600, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.004.h5\n",
      "391/391 [==============================] - 23s 60ms/step - loss: 0.9601 - acc: 0.7161 - val_loss: 1.2047 - val_acc: 0.6460 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 5/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.8759 - acc: 0.7461\n",
      "Epoch 5: val_acc improved from 0.64600 to 0.69900, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.005.h5\n",
      "391/391 [==============================] - 21s 54ms/step - loss: 0.8758 - acc: 0.7462 - val_loss: 1.0139 - val_acc: 0.6990 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 6/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.8127 - acc: 0.7676\n",
      "Epoch 6: val_acc did not improve from 0.69900\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.8127 - acc: 0.7676 - val_loss: 1.3598 - val_acc: 0.6087 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 7/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7655 - acc: 0.7839\n",
      "Epoch 7: val_acc did not improve from 0.69900\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.7655 - acc: 0.7839 - val_loss: 1.3347 - val_acc: 0.6294 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 8/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.7316 - acc: 0.7961\n",
      "Epoch 8: val_acc did not improve from 0.69900\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.7316 - acc: 0.7961 - val_loss: 1.1625 - val_acc: 0.6867 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 9/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6986 - acc: 0.8078\n",
      "Epoch 9: val_acc improved from 0.69900 to 0.72570, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.009.h5\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.6986 - acc: 0.8078 - val_loss: 1.0163 - val_acc: 0.7257 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 10/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6694 - acc: 0.8191\n",
      "Epoch 10: val_acc did not improve from 0.72570\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.6694 - acc: 0.8191 - val_loss: 1.0516 - val_acc: 0.7034 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 11/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6526 - acc: 0.8265\n",
      "Epoch 11: val_acc improved from 0.72570 to 0.77610, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.011.h5\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.6526 - acc: 0.8265 - val_loss: 0.7927 - val_acc: 0.7761 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 12/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6250 - acc: 0.8349\n",
      "Epoch 12: val_acc did not improve from 0.77610\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.6250 - acc: 0.8349 - val_loss: 1.0448 - val_acc: 0.7167 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 13/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.6111 - acc: 0.8396\n",
      "Epoch 13: val_acc improved from 0.77610 to 0.79730, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.013.h5\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.6111 - acc: 0.8396 - val_loss: 0.7460 - val_acc: 0.7973 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 14/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5919 - acc: 0.8459\n",
      "Epoch 14: val_acc did not improve from 0.79730\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.5919 - acc: 0.8459 - val_loss: 0.8638 - val_acc: 0.7669 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 15/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5781 - acc: 0.8501\n",
      "Epoch 15: val_acc did not improve from 0.79730\n",
      "391/391 [==============================] - 21s 52ms/step - loss: 0.5781 - acc: 0.8501 - val_loss: 0.7525 - val_acc: 0.7938 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 16/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5612 - acc: 0.8557\n",
      "Epoch 16: val_acc did not improve from 0.79730\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.5612 - acc: 0.8557 - val_loss: 0.9262 - val_acc: 0.7603 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 17/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5526 - acc: 0.8584\n",
      "Epoch 17: val_acc did not improve from 0.79730\n",
      "391/391 [==============================] - 55s 142ms/step - loss: 0.5526 - acc: 0.8584 - val_loss: 0.9687 - val_acc: 0.7418 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 18/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5384 - acc: 0.8634\n",
      "Epoch 18: val_acc did not improve from 0.79730\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.5384 - acc: 0.8634 - val_loss: 0.7951 - val_acc: 0.7838 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 19/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5349 - acc: 0.8659\n",
      "Epoch 19: val_acc did not improve from 0.79730\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.5349 - acc: 0.8659 - val_loss: 1.0904 - val_acc: 0.7371 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 20/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5245 - acc: 0.8675\n",
      "Epoch 20: val_acc improved from 0.79730 to 0.80650, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.020.h5\n",
      "391/391 [==============================] - 47s 120ms/step - loss: 0.5245 - acc: 0.8675 - val_loss: 0.7146 - val_acc: 0.8065 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 21/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5130 - acc: 0.8729\n",
      "Epoch 21: val_acc did not improve from 0.80650\n",
      "391/391 [==============================] - 48s 122ms/step - loss: 0.5130 - acc: 0.8729 - val_loss: 0.7473 - val_acc: 0.8065 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 22/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.5045 - acc: 0.8757\n",
      "Epoch 22: val_acc did not improve from 0.80650\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.5045 - acc: 0.8757 - val_loss: 0.8891 - val_acc: 0.7766 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 23/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4929 - acc: 0.8812\n",
      "Epoch 23: val_acc did not improve from 0.80650\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.4929 - acc: 0.8812 - val_loss: 0.8089 - val_acc: 0.7997 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 24/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4843 - acc: 0.8834\n",
      "Epoch 24: val_acc did not improve from 0.80650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 46s 118ms/step - loss: 0.4843 - acc: 0.8834 - val_loss: 0.8623 - val_acc: 0.7818 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 25/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4833 - acc: 0.8845\n",
      "Epoch 25: val_acc improved from 0.80650 to 0.81040, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.025.h5\n",
      "391/391 [==============================] - 47s 120ms/step - loss: 0.4833 - acc: 0.8845 - val_loss: 0.7310 - val_acc: 0.8104 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 26/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4767 - acc: 0.8854\n",
      "Epoch 26: val_acc did not improve from 0.81040\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.4767 - acc: 0.8854 - val_loss: 0.9134 - val_acc: 0.7640 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 27/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4743 - acc: 0.8878\n",
      "Epoch 27: val_acc improved from 0.81040 to 0.84030, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.027.h5\n",
      "391/391 [==============================] - 47s 121ms/step - loss: 0.4743 - acc: 0.8878 - val_loss: 0.6437 - val_acc: 0.8403 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 28/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4662 - acc: 0.8898\n",
      "Epoch 28: val_acc did not improve from 0.84030\n",
      "391/391 [==============================] - 46s 119ms/step - loss: 0.4662 - acc: 0.8898 - val_loss: 0.9255 - val_acc: 0.7739 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 29/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4605 - acc: 0.8911\n",
      "Epoch 29: val_acc did not improve from 0.84030\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.4605 - acc: 0.8911 - val_loss: 0.7368 - val_acc: 0.8207 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 30/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4545 - acc: 0.8920\n",
      "Epoch 30: val_acc did not improve from 0.84030\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.4545 - acc: 0.8920 - val_loss: 0.9901 - val_acc: 0.7585 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 31/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4554 - acc: 0.8942\n",
      "Epoch 31: val_acc did not improve from 0.84030\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.4554 - acc: 0.8942 - val_loss: 0.8946 - val_acc: 0.7708 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 32/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4452 - acc: 0.8978\n",
      "Epoch 32: val_acc improved from 0.84030 to 0.84280, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.032.h5\n",
      "391/391 [==============================] - 46s 116ms/step - loss: 0.4452 - acc: 0.8978 - val_loss: 0.6517 - val_acc: 0.8428 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 33/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4488 - acc: 0.8954\n",
      "Epoch 33: val_acc did not improve from 0.84280\n",
      "391/391 [==============================] - 47s 120ms/step - loss: 0.4488 - acc: 0.8954 - val_loss: 1.4032 - val_acc: 0.6876 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 34/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4351 - acc: 0.9010\n",
      "Epoch 34: val_acc did not improve from 0.84280\n",
      "391/391 [==============================] - 46s 119ms/step - loss: 0.4351 - acc: 0.9010 - val_loss: 0.7134 - val_acc: 0.8220 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 35/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4354 - acc: 0.9012\n",
      "Epoch 35: val_acc did not improve from 0.84280\n",
      "391/391 [==============================] - 47s 121ms/step - loss: 0.4354 - acc: 0.9012 - val_loss: 0.7125 - val_acc: 0.8203 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 36/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4356 - acc: 0.9001\n",
      "Epoch 36: val_acc did not improve from 0.84280\n",
      "391/391 [==============================] - 47s 120ms/step - loss: 0.4356 - acc: 0.9001 - val_loss: 0.8676 - val_acc: 0.7953 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 37/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4261 - acc: 0.9055\n",
      "Epoch 37: val_acc did not improve from 0.84280\n",
      "391/391 [==============================] - 48s 121ms/step - loss: 0.4261 - acc: 0.9055 - val_loss: 0.8308 - val_acc: 0.7987 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 38/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4229 - acc: 0.9061\n",
      "Epoch 38: val_acc did not improve from 0.84280\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.4229 - acc: 0.9061 - val_loss: 0.8690 - val_acc: 0.8004 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 39/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4223 - acc: 0.9049\n",
      "Epoch 39: val_acc improved from 0.84280 to 0.84600, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.039.h5\n",
      "391/391 [==============================] - 47s 120ms/step - loss: 0.4223 - acc: 0.9049 - val_loss: 0.6395 - val_acc: 0.8460 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 40/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4172 - acc: 0.9076\n",
      "Epoch 40: val_acc did not improve from 0.84600\n",
      "391/391 [==============================] - 45s 116ms/step - loss: 0.4172 - acc: 0.9076 - val_loss: 0.8740 - val_acc: 0.7968 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 41/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4157 - acc: 0.9075\n",
      "Epoch 41: val_acc did not improve from 0.84600\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.4157 - acc: 0.9075 - val_loss: 0.7588 - val_acc: 0.8271 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 42/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4102 - acc: 0.9103\n",
      "Epoch 42: val_acc did not improve from 0.84600\n",
      "391/391 [==============================] - 46s 119ms/step - loss: 0.4102 - acc: 0.9103 - val_loss: 0.6534 - val_acc: 0.8419 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 43/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4066 - acc: 0.9126\n",
      "Epoch 43: val_acc did not improve from 0.84600\n",
      "391/391 [==============================] - 48s 123ms/step - loss: 0.4066 - acc: 0.9126 - val_loss: 0.9491 - val_acc: 0.7719 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 44/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4070 - acc: 0.9108\n",
      "Epoch 44: val_acc did not improve from 0.84600\n",
      "391/391 [==============================] - 47s 120ms/step - loss: 0.4070 - acc: 0.9108 - val_loss: 0.6428 - val_acc: 0.8445 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 45/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4038 - acc: 0.9126\n",
      "Epoch 45: val_acc improved from 0.84600 to 0.86830, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.045.h5\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.4038 - acc: 0.9126 - val_loss: 0.5568 - val_acc: 0.8683 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 46/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.4011 - acc: 0.9138\n",
      "Epoch 46: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.4011 - acc: 0.9138 - val_loss: 0.7826 - val_acc: 0.8088 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 47/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3948 - acc: 0.9164\n",
      "Epoch 47: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.3948 - acc: 0.9164 - val_loss: 0.7523 - val_acc: 0.8155 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 48/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3959 - acc: 0.9157\n",
      "Epoch 48: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.3959 - acc: 0.9157 - val_loss: 0.8115 - val_acc: 0.8197 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 49/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3940 - acc: 0.9156\n",
      "Epoch 49: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 41s 105ms/step - loss: 0.3940 - acc: 0.9156 - val_loss: 0.7076 - val_acc: 0.8407 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 50/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3918 - acc: 0.9181\n",
      "Epoch 50: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 40s 102ms/step - loss: 0.3918 - acc: 0.9181 - val_loss: 0.8358 - val_acc: 0.8128 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 51/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3874 - acc: 0.9194\n",
      "Epoch 51: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 40s 103ms/step - loss: 0.3874 - acc: 0.9194 - val_loss: 0.6211 - val_acc: 0.8537 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 52/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3867 - acc: 0.9186\n",
      "Epoch 52: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 41s 104ms/step - loss: 0.3867 - acc: 0.9186 - val_loss: 0.6488 - val_acc: 0.8473 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 53/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3877 - acc: 0.9181\n",
      "Epoch 53: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.3877 - acc: 0.9181 - val_loss: 0.7725 - val_acc: 0.8254 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 54/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3833 - acc: 0.9213\n",
      "Epoch 54: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.3833 - acc: 0.9213 - val_loss: 0.7168 - val_acc: 0.8283 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 55/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3844 - acc: 0.9208\n",
      "Epoch 55: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.3844 - acc: 0.9208 - val_loss: 0.8254 - val_acc: 0.8173 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 56/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3819 - acc: 0.9204\n",
      "Epoch 56: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.3819 - acc: 0.9204 - val_loss: 0.7305 - val_acc: 0.8232 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 57/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3785 - acc: 0.9228\n",
      "Epoch 57: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.3785 - acc: 0.9228 - val_loss: 0.7631 - val_acc: 0.8309 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 58/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3778 - acc: 0.9212\n",
      "Epoch 58: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.3778 - acc: 0.9212 - val_loss: 0.5947 - val_acc: 0.8624 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 59/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3795 - acc: 0.9221\n",
      "Epoch 59: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 47s 120ms/step - loss: 0.3795 - acc: 0.9221 - val_loss: 0.7404 - val_acc: 0.8306 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 60/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3763 - acc: 0.9241\n",
      "Epoch 60: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.3763 - acc: 0.9241 - val_loss: 0.6967 - val_acc: 0.8391 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 61/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3727 - acc: 0.9245\n",
      "Epoch 61: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.3727 - acc: 0.9245 - val_loss: 0.7352 - val_acc: 0.8338 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 62/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3710 - acc: 0.9245\n",
      "Epoch 62: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.3710 - acc: 0.9245 - val_loss: 0.7728 - val_acc: 0.8284 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 63/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3713 - acc: 0.9257\n",
      "Epoch 63: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 45s 116ms/step - loss: 0.3713 - acc: 0.9257 - val_loss: 0.7186 - val_acc: 0.8365 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 64/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3770 - acc: 0.9241\n",
      "Epoch 64: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 45s 116ms/step - loss: 0.3770 - acc: 0.9241 - val_loss: 0.7557 - val_acc: 0.8252 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 65/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3697 - acc: 0.9264\n",
      "Epoch 65: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 46s 116ms/step - loss: 0.3697 - acc: 0.9264 - val_loss: 0.6626 - val_acc: 0.8520 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 66/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3619 - acc: 0.9292\n",
      "Epoch 66: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 45s 116ms/step - loss: 0.3619 - acc: 0.9292 - val_loss: 0.8290 - val_acc: 0.8021 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 67/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3691 - acc: 0.9264\n",
      "Epoch 67: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.3691 - acc: 0.9264 - val_loss: 0.7431 - val_acc: 0.8349 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 68/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3700 - acc: 0.9262\n",
      "Epoch 68: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.3700 - acc: 0.9262 - val_loss: 0.7727 - val_acc: 0.8324 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 69/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3644 - acc: 0.9283\n",
      "Epoch 69: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.3644 - acc: 0.9283 - val_loss: 1.0682 - val_acc: 0.7726 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 70/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3650 - acc: 0.9272\n",
      "Epoch 70: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.3650 - acc: 0.9272 - val_loss: 0.7466 - val_acc: 0.8368 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 71/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3600 - acc: 0.9290\n",
      "Epoch 71: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.3600 - acc: 0.9290 - val_loss: 0.7737 - val_acc: 0.8281 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 72/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3587 - acc: 0.9304\n",
      "Epoch 72: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 46s 116ms/step - loss: 0.3587 - acc: 0.9304 - val_loss: 0.6507 - val_acc: 0.8551 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 73/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3603 - acc: 0.9302\n",
      "Epoch 73: val_acc did not improve from 0.86830\n",
      "391/391 [==============================] - 46s 116ms/step - loss: 0.3603 - acc: 0.9302 - val_loss: 0.8365 - val_acc: 0.8185 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 74/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3641 - acc: 0.9292\n",
      "Epoch 74: val_acc improved from 0.86830 to 0.87180, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.074.h5\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.3641 - acc: 0.9292 - val_loss: 0.5537 - val_acc: 0.8718 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 75/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3596 - acc: 0.9296\n",
      "Epoch 75: val_acc did not improve from 0.87180\n",
      "391/391 [==============================] - 45s 116ms/step - loss: 0.3596 - acc: 0.9296 - val_loss: 0.7459 - val_acc: 0.8379 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 76/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3562 - acc: 0.9302\n",
      "Epoch 76: val_acc did not improve from 0.87180\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.3562 - acc: 0.9302 - val_loss: 0.8368 - val_acc: 0.8113 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.001\n",
      "Epoch 77/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3583 - acc: 0.9308\n",
      "Epoch 77: val_acc did not improve from 0.87180\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.3583 - acc: 0.9308 - val_loss: 0.7885 - val_acc: 0.8248 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 78/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3556 - acc: 0.9317\n",
      "Epoch 78: val_acc did not improve from 0.87180\n",
      "391/391 [==============================] - 46s 116ms/step - loss: 0.3556 - acc: 0.9317 - val_loss: 0.6288 - val_acc: 0.8626 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 79/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3545 - acc: 0.9324\n",
      "Epoch 79: val_acc did not improve from 0.87180\n",
      "391/391 [==============================] - 46s 116ms/step - loss: 0.3545 - acc: 0.9324 - val_loss: 0.7661 - val_acc: 0.8333 - lr: 3.1623e-04\n",
      "Learning rate:  0.001\n",
      "Epoch 80/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3582 - acc: 0.9311\n",
      "Epoch 80: val_acc did not improve from 0.87180\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.3582 - acc: 0.9311 - val_loss: 0.6903 - val_acc: 0.8399 - lr: 0.0010\n",
      "Learning rate:  0.001\n",
      "Epoch 81/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.3520 - acc: 0.9337\n",
      "Epoch 81: val_acc did not improve from 0.87180\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.3520 - acc: 0.9337 - val_loss: 0.6951 - val_acc: 0.8446 - lr: 0.0010\n",
      "Learning rate:  0.0001\n",
      "Epoch 82/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2917 - acc: 0.9558\n",
      "Epoch 82: val_acc improved from 0.87180 to 0.89930, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.082.h5\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.2917 - acc: 0.9558 - val_loss: 0.4914 - val_acc: 0.8993 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 83/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2697 - acc: 0.9632\n",
      "Epoch 83: val_acc did not improve from 0.89930\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.2697 - acc: 0.9632 - val_loss: 0.4995 - val_acc: 0.8987 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 84/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2586 - acc: 0.9660\n",
      "Epoch 84: val_acc improved from 0.89930 to 0.90330, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.084.h5\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.2586 - acc: 0.9660 - val_loss: 0.4891 - val_acc: 0.9033 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 85/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2517 - acc: 0.9689\n",
      "Epoch 85: val_acc did not improve from 0.90330\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.2517 - acc: 0.9689 - val_loss: 0.4855 - val_acc: 0.9029 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 86/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2450 - acc: 0.9714\n",
      "Epoch 86: val_acc did not improve from 0.90330\n",
      "391/391 [==============================] - 45s 116ms/step - loss: 0.2450 - acc: 0.9714 - val_loss: 0.4832 - val_acc: 0.9028 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 87/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2412 - acc: 0.9714\n",
      "Epoch 87: val_acc did not improve from 0.90330\n",
      "391/391 [==============================] - 45s 116ms/step - loss: 0.2412 - acc: 0.9714 - val_loss: 0.4923 - val_acc: 0.9012 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 88/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2354 - acc: 0.9734\n",
      "Epoch 88: val_acc did not improve from 0.90330\n",
      "391/391 [==============================] - 45s 116ms/step - loss: 0.2354 - acc: 0.9734 - val_loss: 0.4955 - val_acc: 0.9025 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 89/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2320 - acc: 0.9734\n",
      "Epoch 89: val_acc did not improve from 0.90330\n",
      "391/391 [==============================] - 46s 117ms/step - loss: 0.2320 - acc: 0.9734 - val_loss: 0.4938 - val_acc: 0.9008 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 90/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2288 - acc: 0.9746\n",
      "Epoch 90: val_acc improved from 0.90330 to 0.90360, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.090.h5\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.2288 - acc: 0.9746 - val_loss: 0.4886 - val_acc: 0.9036 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 91/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2267 - acc: 0.9748\n",
      "Epoch 91: val_acc improved from 0.90360 to 0.90530, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.091.h5\n",
      "391/391 [==============================] - 46s 118ms/step - loss: 0.2267 - acc: 0.9748 - val_loss: 0.4798 - val_acc: 0.9053 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 92/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2196 - acc: 0.9776\n",
      "Epoch 92: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.2196 - acc: 0.9776 - val_loss: 0.4955 - val_acc: 0.9044 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 93/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2192 - acc: 0.9766\n",
      "Epoch 93: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.2192 - acc: 0.9766 - val_loss: 0.4872 - val_acc: 0.9033 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 94/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2174 - acc: 0.9767\n",
      "Epoch 94: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.2174 - acc: 0.9767 - val_loss: 0.4833 - val_acc: 0.9043 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 95/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2143 - acc: 0.9782\n",
      "Epoch 95: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 116ms/step - loss: 0.2143 - acc: 0.9782 - val_loss: 0.4984 - val_acc: 0.9031 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 96/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2112 - acc: 0.9789\n",
      "Epoch 96: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 116ms/step - loss: 0.2112 - acc: 0.9789 - val_loss: 0.4937 - val_acc: 0.9035 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 97/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2078 - acc: 0.9793\n",
      "Epoch 97: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 47s 120ms/step - loss: 0.2078 - acc: 0.9793 - val_loss: 0.4921 - val_acc: 0.9031 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 98/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2062 - acc: 0.9802\n",
      "Epoch 98: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.2062 - acc: 0.9802 - val_loss: 0.5140 - val_acc: 0.9024 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 99/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2044 - acc: 0.9803\n",
      "Epoch 99: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 116ms/step - loss: 0.2044 - acc: 0.9803 - val_loss: 0.5020 - val_acc: 0.9033 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 100/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.2030 - acc: 0.9803\n",
      "Epoch 100: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 116ms/step - loss: 0.2030 - acc: 0.9803 - val_loss: 0.5007 - val_acc: 0.9022 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 101/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1987 - acc: 0.9818\n",
      "Epoch 101: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.1987 - acc: 0.9818 - val_loss: 0.5073 - val_acc: 0.9008 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 102/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - ETA: 0s - loss: 0.1961 - acc: 0.9822\n",
      "Epoch 102: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1961 - acc: 0.9822 - val_loss: 0.5248 - val_acc: 0.8996 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 103/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1966 - acc: 0.9812\n",
      "Epoch 103: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 44s 113ms/step - loss: 0.1966 - acc: 0.9812 - val_loss: 0.5027 - val_acc: 0.9014 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 104/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1939 - acc: 0.9815\n",
      "Epoch 104: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 116ms/step - loss: 0.1939 - acc: 0.9815 - val_loss: 0.4911 - val_acc: 0.9044 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 105/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1915 - acc: 0.9826\n",
      "Epoch 105: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.1915 - acc: 0.9826 - val_loss: 0.5113 - val_acc: 0.9024 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 106/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1898 - acc: 0.9827\n",
      "Epoch 106: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.1898 - acc: 0.9827 - val_loss: 0.5078 - val_acc: 0.9014 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 107/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1898 - acc: 0.9819\n",
      "Epoch 107: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.1898 - acc: 0.9819 - val_loss: 0.5055 - val_acc: 0.9029 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 108/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1859 - acc: 0.9837\n",
      "Epoch 108: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.1859 - acc: 0.9837 - val_loss: 0.5142 - val_acc: 0.9032 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 109/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1865 - acc: 0.9832\n",
      "Epoch 109: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.1865 - acc: 0.9832 - val_loss: 0.5080 - val_acc: 0.9043 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 110/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1843 - acc: 0.9838\n",
      "Epoch 110: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.1843 - acc: 0.9838 - val_loss: 0.5278 - val_acc: 0.8996 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 111/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1808 - acc: 0.9840\n",
      "Epoch 111: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.1808 - acc: 0.9840 - val_loss: 0.5118 - val_acc: 0.9039 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 112/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1814 - acc: 0.9837\n",
      "Epoch 112: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 114ms/step - loss: 0.1814 - acc: 0.9837 - val_loss: 0.5224 - val_acc: 0.9028 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 113/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1806 - acc: 0.9836\n",
      "Epoch 113: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.1806 - acc: 0.9836 - val_loss: 0.5088 - val_acc: 0.9035 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 114/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1769 - acc: 0.9854\n",
      "Epoch 114: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 45s 115ms/step - loss: 0.1769 - acc: 0.9854 - val_loss: 0.5144 - val_acc: 0.9040 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 115/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1769 - acc: 0.9848\n",
      "Epoch 115: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 44s 112ms/step - loss: 0.1769 - acc: 0.9848 - val_loss: 0.5146 - val_acc: 0.9019 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 116/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1754 - acc: 0.9853\n",
      "Epoch 116: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.1754 - acc: 0.9853 - val_loss: 0.5401 - val_acc: 0.9001 - lr: 3.1623e-05\n",
      "Learning rate:  0.0001\n",
      "Epoch 117/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1745 - acc: 0.9856\n",
      "Epoch 117: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1745 - acc: 0.9856 - val_loss: 0.5314 - val_acc: 0.9011 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 118/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1720 - acc: 0.9850\n",
      "Epoch 118: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1720 - acc: 0.9850 - val_loss: 0.5286 - val_acc: 0.9019 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 119/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1710 - acc: 0.9860\n",
      "Epoch 119: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1711 - acc: 0.9859 - val_loss: 0.5149 - val_acc: 0.9018 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 120/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.9858\n",
      "Epoch 120: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1707 - acc: 0.9858 - val_loss: 0.5204 - val_acc: 0.9025 - lr: 1.0000e-04\n",
      "Learning rate:  0.0001\n",
      "Epoch 121/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1685 - acc: 0.9870\n",
      "Epoch 121: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1685 - acc: 0.9870 - val_loss: 0.5437 - val_acc: 0.9019 - lr: 3.1623e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 122/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1650 - acc: 0.9872\n",
      "Epoch 122: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1650 - acc: 0.9872 - val_loss: 0.5130 - val_acc: 0.9040 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 123/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1628 - acc: 0.9882\n",
      "Epoch 123: val_acc did not improve from 0.90530\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1628 - acc: 0.9882 - val_loss: 0.5092 - val_acc: 0.9052 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 124/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1618 - acc: 0.9888\n",
      "Epoch 124: val_acc improved from 0.90530 to 0.90560, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.124.h5\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1617 - acc: 0.9888 - val_loss: 0.5094 - val_acc: 0.9056 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 125/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1614 - acc: 0.9893\n",
      "Epoch 125: val_acc improved from 0.90560 to 0.90580, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.125.h5\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.1614 - acc: 0.9893 - val_loss: 0.5098 - val_acc: 0.9058 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 126/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.9893\n",
      "Epoch 126: val_acc improved from 0.90580 to 0.90590, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.126.h5\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.1601 - acc: 0.9893 - val_loss: 0.5066 - val_acc: 0.9059 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 127/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1599 - acc: 0.9895\n",
      "Epoch 127: val_acc did not improve from 0.90590\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1599 - acc: 0.9894 - val_loss: 0.5076 - val_acc: 0.9054 - lr: 1.0000e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  1e-05\n",
      "Epoch 128/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1594 - acc: 0.9899\n",
      "Epoch 128: val_acc did not improve from 0.90590\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1594 - acc: 0.9899 - val_loss: 0.5094 - val_acc: 0.9059 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 129/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9897\n",
      "Epoch 129: val_acc did not improve from 0.90590\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1586 - acc: 0.9898 - val_loss: 0.5082 - val_acc: 0.9044 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 130/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1586 - acc: 0.9900\n",
      "Epoch 130: val_acc did not improve from 0.90590\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1586 - acc: 0.9900 - val_loss: 0.5112 - val_acc: 0.9055 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 131/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1580 - acc: 0.9900\n",
      "Epoch 131: val_acc did not improve from 0.90590\n",
      "391/391 [==============================] - 39s 99ms/step - loss: 0.1580 - acc: 0.9900 - val_loss: 0.5108 - val_acc: 0.9051 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 132/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.9899\n",
      "Epoch 132: val_acc improved from 0.90590 to 0.90630, saving model to D:\\Data science\\Machine Learning\\Deep learning\\Tensorflow - new\\saved_models\\cifar10_ResNet20v1_model.132.h5\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1585 - acc: 0.9899 - val_loss: 0.5072 - val_acc: 0.9063 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 133/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1579 - acc: 0.9895\n",
      "Epoch 133: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1579 - acc: 0.9895 - val_loss: 0.5072 - val_acc: 0.9046 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 134/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.9893\n",
      "Epoch 134: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 21s 53ms/step - loss: 0.1584 - acc: 0.9893 - val_loss: 0.5088 - val_acc: 0.9047 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 135/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1580 - acc: 0.9900\n",
      "Epoch 135: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 23s 58ms/step - loss: 0.1580 - acc: 0.9900 - val_loss: 0.5100 - val_acc: 0.9048 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 136/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1574 - acc: 0.9901\n",
      "Epoch 136: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 22s 56ms/step - loss: 0.1574 - acc: 0.9901 - val_loss: 0.5091 - val_acc: 0.9048 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 137/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1565 - acc: 0.9903\n",
      "Epoch 137: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1565 - acc: 0.9903 - val_loss: 0.5087 - val_acc: 0.9035 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 138/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1575 - acc: 0.9896\n",
      "Epoch 138: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1575 - acc: 0.9896 - val_loss: 0.5120 - val_acc: 0.9044 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 139/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1549 - acc: 0.9913\n",
      "Epoch 139: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1549 - acc: 0.9913 - val_loss: 0.5122 - val_acc: 0.9040 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 140/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1547 - acc: 0.9909\n",
      "Epoch 140: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 50ms/step - loss: 0.1547 - acc: 0.9909 - val_loss: 0.5121 - val_acc: 0.9037 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 141/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1564 - acc: 0.9906\n",
      "Epoch 141: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1563 - acc: 0.9906 - val_loss: 0.5117 - val_acc: 0.9036 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 142/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1561 - acc: 0.9909\n",
      "Epoch 142: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1561 - acc: 0.9909 - val_loss: 0.5106 - val_acc: 0.9032 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 143/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1551 - acc: 0.9907\n",
      "Epoch 143: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1551 - acc: 0.9907 - val_loss: 0.5122 - val_acc: 0.9049 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 144/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1551 - acc: 0.9907\n",
      "Epoch 144: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.1551 - acc: 0.9907 - val_loss: 0.5125 - val_acc: 0.9044 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 145/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9906\n",
      "Epoch 145: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1546 - acc: 0.9906 - val_loss: 0.5145 - val_acc: 0.9048 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 146/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1548 - acc: 0.9906\n",
      "Epoch 146: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1548 - acc: 0.9906 - val_loss: 0.5122 - val_acc: 0.9045 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 147/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1547 - acc: 0.9909\n",
      "Epoch 147: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1548 - acc: 0.9909 - val_loss: 0.5163 - val_acc: 0.9040 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 148/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1530 - acc: 0.9919\n",
      "Epoch 148: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1530 - acc: 0.9919 - val_loss: 0.5123 - val_acc: 0.9041 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 149/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.9911\n",
      "Epoch 149: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1531 - acc: 0.9911 - val_loss: 0.5135 - val_acc: 0.9039 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 150/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1532 - acc: 0.9910\n",
      "Epoch 150: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1532 - acc: 0.9910 - val_loss: 0.5140 - val_acc: 0.9039 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 151/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1535 - acc: 0.9907\n",
      "Epoch 151: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1535 - acc: 0.9907 - val_loss: 0.5172 - val_acc: 0.9042 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 152/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1540 - acc: 0.9908\n",
      "Epoch 152: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1540 - acc: 0.9907 - val_loss: 0.5169 - val_acc: 0.9036 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 153/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1531 - acc: 0.9909\n",
      "Epoch 153: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1531 - acc: 0.9909 - val_loss: 0.5138 - val_acc: 0.9041 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 154/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1525 - acc: 0.9917\n",
      "Epoch 154: val_acc did not improve from 0.90630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1525 - acc: 0.9917 - val_loss: 0.5175 - val_acc: 0.9038 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 155/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.9913\n",
      "Epoch 155: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1525 - acc: 0.9913 - val_loss: 0.5155 - val_acc: 0.9044 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 156/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1517 - acc: 0.9921\n",
      "Epoch 156: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1517 - acc: 0.9921 - val_loss: 0.5156 - val_acc: 0.9030 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 157/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1530 - acc: 0.9908\n",
      "Epoch 157: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1530 - acc: 0.9908 - val_loss: 0.5142 - val_acc: 0.9036 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 158/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.9914\n",
      "Epoch 158: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1523 - acc: 0.9914 - val_loss: 0.5156 - val_acc: 0.9032 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 159/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9910\n",
      "Epoch 159: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1529 - acc: 0.9910 - val_loss: 0.5162 - val_acc: 0.9041 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 160/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1513 - acc: 0.9917\n",
      "Epoch 160: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1513 - acc: 0.9917 - val_loss: 0.5156 - val_acc: 0.9037 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 161/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9921\n",
      "Epoch 161: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1504 - acc: 0.9921 - val_loss: 0.5155 - val_acc: 0.9033 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 162/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1502 - acc: 0.9915\n",
      "Epoch 162: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1502 - acc: 0.9915 - val_loss: 0.5159 - val_acc: 0.9031 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 163/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1524 - acc: 0.9907\n",
      "Epoch 163: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1524 - acc: 0.9907 - val_loss: 0.5166 - val_acc: 0.9037 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 164/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.9917\n",
      "Epoch 164: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1513 - acc: 0.9917 - val_loss: 0.5158 - val_acc: 0.9036 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 165/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.9917\n",
      "Epoch 165: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1503 - acc: 0.9917 - val_loss: 0.5186 - val_acc: 0.9042 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 166/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1514 - acc: 0.9912\n",
      "Epoch 166: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1514 - acc: 0.9912 - val_loss: 0.5174 - val_acc: 0.9023 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 167/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1505 - acc: 0.9916\n",
      "Epoch 167: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1505 - acc: 0.9916 - val_loss: 0.5173 - val_acc: 0.9034 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 168/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1499 - acc: 0.9916\n",
      "Epoch 168: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1499 - acc: 0.9916 - val_loss: 0.5169 - val_acc: 0.9030 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 169/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1516 - acc: 0.9912\n",
      "Epoch 169: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1516 - acc: 0.9912 - val_loss: 0.5154 - val_acc: 0.9038 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 170/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1496 - acc: 0.9922\n",
      "Epoch 170: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1496 - acc: 0.9922 - val_loss: 0.5164 - val_acc: 0.9031 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 171/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.9914\n",
      "Epoch 171: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1506 - acc: 0.9913 - val_loss: 0.5212 - val_acc: 0.9029 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 172/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1494 - acc: 0.9917\n",
      "Epoch 172: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1494 - acc: 0.9917 - val_loss: 0.5160 - val_acc: 0.9040 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 173/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.9922\n",
      "Epoch 173: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1491 - acc: 0.9922 - val_loss: 0.5162 - val_acc: 0.9037 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 174/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1492 - acc: 0.9918\n",
      "Epoch 174: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1492 - acc: 0.9918 - val_loss: 0.5156 - val_acc: 0.9035 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 175/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1491 - acc: 0.9917\n",
      "Epoch 175: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1491 - acc: 0.9917 - val_loss: 0.5154 - val_acc: 0.9032 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 176/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1493 - acc: 0.9917\n",
      "Epoch 176: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1493 - acc: 0.9917 - val_loss: 0.5194 - val_acc: 0.9033 - lr: 3.1623e-06\n",
      "Learning rate:  1e-05\n",
      "Epoch 177/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.9915\n",
      "Epoch 177: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.1486 - acc: 0.9915 - val_loss: 0.5188 - val_acc: 0.9027 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 178/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1500 - acc: 0.9909\n",
      "Epoch 178: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1500 - acc: 0.9909 - val_loss: 0.5186 - val_acc: 0.9038 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 179/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1483 - acc: 0.9922\n",
      "Epoch 179: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1483 - acc: 0.9922 - val_loss: 0.5150 - val_acc: 0.9040 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 180/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9918\n",
      "Epoch 180: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1484 - acc: 0.9919 - val_loss: 0.5185 - val_acc: 0.9041 - lr: 1.0000e-05\n",
      "Learning rate:  1e-05\n",
      "Epoch 181/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9918\n",
      "Epoch 181: val_acc did not improve from 0.90630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1484 - acc: 0.9918 - val_loss: 0.5174 - val_acc: 0.9037 - lr: 3.1623e-06\n",
      "Learning rate:  5e-07\n",
      "Epoch 182/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1485 - acc: 0.9919\n",
      "Epoch 182: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1485 - acc: 0.9919 - val_loss: 0.5180 - val_acc: 0.9031 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 183/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9919\n",
      "Epoch 183: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.1480 - acc: 0.9919 - val_loss: 0.5166 - val_acc: 0.9035 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 184/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.9919\n",
      "Epoch 184: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1484 - acc: 0.9919 - val_loss: 0.5185 - val_acc: 0.9033 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 185/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.9922\n",
      "Epoch 185: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1478 - acc: 0.9922 - val_loss: 0.5175 - val_acc: 0.9035 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 186/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1485 - acc: 0.9914\n",
      "Epoch 186: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1485 - acc: 0.9914 - val_loss: 0.5168 - val_acc: 0.9037 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 187/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1486 - acc: 0.9921\n",
      "Epoch 187: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.1486 - acc: 0.9921 - val_loss: 0.5176 - val_acc: 0.9035 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 188/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.9925\n",
      "Epoch 188: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1473 - acc: 0.9925 - val_loss: 0.5172 - val_acc: 0.9032 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 189/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.9917\n",
      "Epoch 189: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.1481 - acc: 0.9918 - val_loss: 0.5176 - val_acc: 0.9029 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 190/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1480 - acc: 0.9921\n",
      "Epoch 190: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1481 - acc: 0.9920 - val_loss: 0.5176 - val_acc: 0.9031 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 191/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1493 - acc: 0.9918\n",
      "Epoch 191: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1492 - acc: 0.9918 - val_loss: 0.5170 - val_acc: 0.9034 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 192/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.9920\n",
      "Epoch 192: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1484 - acc: 0.9920 - val_loss: 0.5166 - val_acc: 0.9035 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 193/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1474 - acc: 0.9919\n",
      "Epoch 193: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1474 - acc: 0.9919 - val_loss: 0.5172 - val_acc: 0.9028 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 194/200\n",
      "390/391 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9914\n",
      "Epoch 194: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1488 - acc: 0.9914 - val_loss: 0.5176 - val_acc: 0.9035 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 195/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1479 - acc: 0.9920\n",
      "Epoch 195: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1479 - acc: 0.9920 - val_loss: 0.5173 - val_acc: 0.9033 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 196/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1471 - acc: 0.9924\n",
      "Epoch 196: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 52ms/step - loss: 0.1471 - acc: 0.9924 - val_loss: 0.5169 - val_acc: 0.9033 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 197/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1469 - acc: 0.9923\n",
      "Epoch 197: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1469 - acc: 0.9923 - val_loss: 0.5173 - val_acc: 0.9033 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 198/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1485 - acc: 0.9915\n",
      "Epoch 198: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1485 - acc: 0.9915 - val_loss: 0.5170 - val_acc: 0.9033 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 199/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1488 - acc: 0.9914\n",
      "Epoch 199: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1488 - acc: 0.9914 - val_loss: 0.5175 - val_acc: 0.9035 - lr: 5.0000e-07\n",
      "Learning rate:  5e-07\n",
      "Epoch 200/200\n",
      "391/391 [==============================] - ETA: 0s - loss: 0.1486 - acc: 0.9917\n",
      "Epoch 200: val_acc did not improve from 0.90630\n",
      "391/391 [==============================] - 20s 51ms/step - loss: 0.1486 - acc: 0.9917 - val_loss: 0.5177 - val_acc: 0.9026 - lr: 5.0000e-07\n"
     ]
    }
   ],
   "source": [
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,\n",
    "              callbacks=callbacks)\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # this will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=0,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False)\n",
    "\n",
    "    # compute quantities required for featurewise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    steps_per_epoch =  math.ceil(len(x_train) / batch_size)\n",
    "    # fit the model on the batches generated by datagen.flow().\n",
    "    model.fit(x=datagen.flow(x_train, y_train, batch_size=batch_size),\n",
    "              verbose=1,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              steps_per_epoch=steps_per_epoch,\n",
    "              callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b9c42f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.5176630616188049\n",
      "Test accuracy: 0.9025999903678894\n"
     ]
    }
   ],
   "source": [
    "# score trained model\n",
    "scores = model.evaluate(x_test,\n",
    "                        y_test,\n",
    "                        batch_size=batch_size,\n",
    "                        verbose=0)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57fbca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
